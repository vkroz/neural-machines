{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkroz/neural-machines/blob/main/regression%20101/regression%20-%20home%20prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2acf6b5b12f3ccba",
      "metadata": {
        "id": "2acf6b5b12f3ccba"
      },
      "source": [
        "# End-to-End Linear Regression Example: Home Price Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWHIMZYSD13z"
      },
      "id": "lWHIMZYSD13z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c4c0956",
      "metadata": {
        "id": "9c4c0956"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1371f1cd",
      "metadata": {
        "id": "1371f1cd",
        "outputId": "baf7e0dd-cfd3-4f51-ac8f-72c25b23fd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving notices: done\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: osx-arm64\n",
            "Collecting package metadata (repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!conda install datasets scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7d2419",
      "metadata": {
        "id": "af7d2419",
        "outputId": "f7bcd3ac-423a-4e62-e4b8-d25970512704"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efcabb0f",
      "metadata": {
        "id": "efcabb0f"
      },
      "source": [
        "## 2. Loading Data from Hugging Face\n",
        "\n",
        "We'll use the 'house_prices' dataset from Hugging Face, which contains information about house features and their prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf40539",
      "metadata": {
        "id": "fcf40539",
        "outputId": "8731c98e-1434-4066-9fd8-84edee8e4b73"
      },
      "outputs": [
        {
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'maharshipandya/house-prices' doesn't exist on the Hub or cannot be accessed.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset from Hugging Face\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaharshipandya/house-prices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset structure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/datasets/load.py:1719\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
            "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/datasets/load.py:1645\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1643\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1647\u001b[0m     dataset_script_path \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m   1648\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1649\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m   1653\u001b[0m     )\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'maharshipandya/house-prices' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ],
      "source": [
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"maharshipandya/house-prices\")\n",
        "print(f\"Dataset structure: {dataset}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c751239d",
      "metadata": {
        "id": "c751239d"
      },
      "outputs": [],
      "source": [
        "# Convert to pandas DataFrame for easier manipulation\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976c4930",
      "metadata": {
        "id": "976c4930"
      },
      "source": [
        "## 3. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e09ec3d",
      "metadata": {
        "id": "3e09ec3d"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values[missing_values > 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370ae460",
      "metadata": {
        "id": "370ae460"
      },
      "outputs": [],
      "source": [
        "# Basic statistics of numerical features\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16801a5",
      "metadata": {
        "id": "c16801a5"
      },
      "outputs": [],
      "source": [
        "# Distribution of the target variable (SalePrice)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['SalePrice'], kde=True)\n",
        "plt.title('Distribution of House Prices')\n",
        "plt.xlabel('Price ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5398834",
      "metadata": {
        "id": "a5398834"
      },
      "outputs": [],
      "source": [
        "# Correlation between numerical features and the target\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "correlation = df[numerical_features].corr()['SalePrice'].sort_values(ascending=False)\n",
        "print(\"Top 10 features correlated with SalePrice:\")\n",
        "print(correlation[:11])  # Including SalePrice itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76817c4c",
      "metadata": {
        "id": "76817c4c"
      },
      "outputs": [],
      "source": [
        "# Visualize the top 5 correlated features with SalePrice\n",
        "top_features = correlation[1:6].index  # Exclude SalePrice itself\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    sns.scatterplot(x=feature, y='SalePrice', data=df, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature} vs SalePrice')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c03756",
      "metadata": {
        "id": "b2c03756"
      },
      "source": [
        "## 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae95ac3",
      "metadata": {
        "id": "fae95ac3"
      },
      "outputs": [],
      "source": [
        "# Select features based on correlation analysis\n",
        "# We'll use the top correlated numerical features for simplicity\n",
        "selected_features = correlation[1:6].index.tolist()\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "\n",
        "# Prepare the data\n",
        "X = df[selected_features]\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e036d8",
      "metadata": {
        "id": "d9e036d8"
      },
      "source": [
        "## 5. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc7779d",
      "metadata": {
        "id": "5cc7779d"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline with preprocessing and model\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize features\n",
        "    ('regressor', LinearRegression())  # Linear regression model\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients = pipeline.named_steps['regressor'].coef_\n",
        "intercept = pipeline.named_steps['regressor'].intercept_\n",
        "\n",
        "# Display the model coefficients\n",
        "coef_df = pd.DataFrame({'Feature': selected_features, 'Coefficient': coefficients})\n",
        "print(\"Model Coefficients:\")\n",
        "print(coef_df)\n",
        "print(f\"Intercept: {intercept:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb54990b",
      "metadata": {
        "id": "eb54990b"
      },
      "source": [
        "## 6. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b95f28",
      "metadata": {
        "id": "82b95f28"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b223632d",
      "metadata": {
        "id": "b223632d"
      },
      "outputs": [],
      "source": [
        "# Visualize actual vs predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Actual vs Predicted House Prices')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54b0e90",
      "metadata": {
        "id": "a54b0e90"
      },
      "outputs": [],
      "source": [
        "# Plot residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Prices')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c50c075",
      "metadata": {
        "id": "2c50c075"
      },
      "source": [
        "## 7. Model Inference\n",
        "\n",
        "Now let's use our trained model to make predictions on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa47f14f",
      "metadata": {
        "id": "fa47f14f"
      },
      "outputs": [],
      "source": [
        "# Create a function for making predictions\n",
        "def predict_house_price(features_dict):\n",
        "    # Convert input dictionary to DataFrame\n",
        "    input_df = pd.DataFrame([features_dict])\n",
        "\n",
        "    # Ensure all required features are present\n",
        "    for feature in selected_features:\n",
        "        if feature not in input_df.columns:\n",
        "            raise ValueError(f\"Missing required feature: {feature}\")\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_price = pipeline.predict(input_df[selected_features])[0]\n",
        "    return predicted_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed23442e",
      "metadata": {
        "id": "ed23442e"
      },
      "outputs": [],
      "source": [
        "# Example: Predict prices for sample houses\n",
        "# We'll use the median values from our dataset as a starting point\n",
        "sample_house = {}\n",
        "for feature in selected_features:\n",
        "    sample_house[feature] = df[feature].median()\n",
        "\n",
        "print(\"Sample house features:\")\n",
        "for feature, value in sample_house.items():\n",
        "    print(f\"{feature}: {value}\")\n",
        "\n",
        "predicted_price = predict_house_price(sample_house)\n",
        "print(f\"\\nPredicted house price: ${predicted_price:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3060ae30",
      "metadata": {
        "id": "3060ae30"
      },
      "outputs": [],
      "source": [
        "# Let's try with different values\n",
        "# Create a more expensive house by increasing the values by 20%\n",
        "expensive_house = {}\n",
        "for feature in selected_features:\n",
        "    expensive_house[feature] = df[feature].median() * 1.2\n",
        "\n",
        "print(\"Expensive house features:\")\n",
        "for feature, value in expensive_house.items():\n",
        "    print(f\"{feature}: {value}\")\n",
        "\n",
        "predicted_price = predict_house_price(expensive_house)\n",
        "print(f\"\\nPredicted house price: ${predicted_price:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbaad17",
      "metadata": {
        "id": "ebbaad17"
      },
      "outputs": [],
      "source": [
        "# Create a less expensive house by decreasing the values by 20%\n",
        "cheaper_house = {}\n",
        "for feature in selected_features:\n",
        "    cheaper_house[feature] = df[feature].median() * 0.8\n",
        "\n",
        "print(\"Cheaper house features:\")\n",
        "for feature, value in cheaper_house.items():\n",
        "    print(f\"{feature}: {value}\")\n",
        "\n",
        "predicted_price = predict_house_price(cheaper_house)\n",
        "print(f\"\\nPredicted house price: ${predicted_price:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f5b929",
      "metadata": {
        "id": "11f5b929"
      },
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated a complete machine learning lifecycle for a linear regression model to predict house prices:\n",
        "\n",
        "1. **Data Loading**: We loaded a house prices dataset from Hugging Face.\n",
        "2. **Exploratory Data Analysis**: We analyzed the dataset to understand its structure and relationships.\n",
        "3. **Feature Selection**: We selected the most relevant features based on correlation with the target variable.\n",
        "4. **Data Preprocessing**: We split the data and standardized the features.\n",
        "5. **Model Training**: We trained a linear regression model using scikit-learn.\n",
        "6. **Model Evaluation**: We evaluated the model using various metrics (RMSE, MAE, R²).\n",
        "7. **Model Inference**: We used the trained model to make predictions on new data.\n",
        "\n",
        "This simple example demonstrates the fundamental steps in a machine learning project. For a real-world application, you might want to consider:\n",
        "- More sophisticated feature engineering\n",
        "- Handling categorical variables\n",
        "- Addressing outliers and missing values more thoroughly\n",
        "- Trying more complex models\n",
        "- Implementing cross-validation\n",
        "- Model deployment strategies"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "neumans",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}