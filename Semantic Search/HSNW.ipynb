{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step-by-Step Guide\n",
    "\n",
    "Readme in `hnswlib` repository provides comprehensive selfcontained guide to use the library. Here is a step-by-step guide to use the library.\n",
    "https://github.com/nmslib/hnswlib/blob/master/README.md\n"
   ],
   "id": "5f428acc4fd60e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:38:20.188493Z",
     "start_time": "2024-05-17T23:38:19.299853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dim = 128\n",
    "num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "data = np.float32(np.random.random((num_elements, dim)))\n",
    "ids = np.arange(num_elements)\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space = 'l2', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "# Initializing index - the maximum number of elements should be known beforehand\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "\n",
    "# Element insertion (can be called several times):\n",
    "p.add_items(data, ids)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "p.set_ef(50) # ef should always be > k\n",
    "\n",
    "# Query dataset, k - number of the closest elements (returns 2 numpy arrays)\n",
    "labels, distances = p.knn_query(data, k = 1)\n",
    "\n",
    "# Index objects support pickling\n",
    "# WARNING: serialization via pickle.dumps(p) or p.__getstate__() is NOT thread-safe with p.add_items method!\n",
    "# Note: ef parameter is included in serialization; random number generator is initialized with random_seed on Index load\n",
    "p_copy = pickle.loads(pickle.dumps(p)) # creates a copy of index p using pickle round-trip\n",
    "\n",
    "### Index parameters are exposed as class properties:\n",
    "print(f\"Parameters passed to constructor:  space={p_copy.space}, dim={p_copy.dim}\")\n",
    "print(f\"Index construction: M={p_copy.M}, ef_construction={p_copy.ef_construction}\")\n",
    "print(f\"Index size is {p_copy.element_count} and index capacity is {p_copy.max_elements}\")\n",
    "print(f\"Search speed/quality trade-off parameter: ef={p_copy.ef}\")"
   ],
   "id": "2f4390bf743b682b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters passed to constructor:  space=l2, dim=128\n",
      "Index construction: M=16, ef_construction=200\n",
      "Index size is 10000 and index capacity is 10000\n",
      "Search speed/quality trade-off parameter: ef=50\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example with updates after serialization/deserialization:\n",
    "\n"
   ],
   "id": "195e00ffe498db56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:38:44.391856Z",
     "start_time": "2024-05-17T23:38:43.988783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = 16\n",
    "num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "data = np.float32(np.random.random((num_elements, dim)))\n",
    "\n",
    "# We split the data in two batches:\n",
    "data1 = data[:num_elements // 2]\n",
    "data2 = data[num_elements // 2:]\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip\n",
    "\n",
    "# Initializing index\n",
    "# max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "# during insertion of an element.\n",
    "# The capacity can be increased by saving/loading the index, see below.\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "#\n",
    "# M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n",
    "# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "\n",
    "p.init_index(max_elements=num_elements//2, ef_construction=100, M=16)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "p.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "p.set_num_threads(4)\n",
    "\n",
    "print(\"Adding first batch of %d elements\" % (len(data1)))\n",
    "p.add_items(data1)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data1, k=1)\n",
    "print(\"Recall for the first batch:\", np.mean(labels.reshape(-1) == np.arange(len(data1))), \"\\n\")\n",
    "\n",
    "# Serializing and deleting the index:\n",
    "index_path='first_half.bin'\n",
    "print(\"Saving index to '%s'\" % index_path)\n",
    "p.save_index(\"first_half.bin\")\n",
    "del p\n",
    "\n",
    "# Re-initializing, loading the index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.\n",
    "\n",
    "print(\"\\nLoading index from 'first_half.bin'\\n\")\n",
    "\n",
    "# Increase the total capacity (max_elements), so that it will handle the new data\n",
    "p.load_index(\"first_half.bin\", max_elements = num_elements)\n",
    "\n",
    "print(\"Adding the second batch of %d elements\" % (len(data2)))\n",
    "p.add_items(data2)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data, k=1)\n",
    "print(\"Recall for two batches:\", np.mean(labels.reshape(-1) == np.arange(len(data))), \"\\n\")"
   ],
   "id": "4e11c3bd359877c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding first batch of 5000 elements\n",
      "Recall for the first batch: 1.0 \n",
      "\n",
      "Saving index to 'first_half.bin'\n",
      "\n",
      "Loading index from 'first_half.bin'\n",
      "\n",
      "Adding the second batch of 5000 elements\n",
      "Recall for two batches: 1.0 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Creating index, inserting elements, searching and pickle serialization:\n",
    "\n"
   ],
   "id": "406a68501b5e6cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:39:26.747686Z",
     "start_time": "2024-05-17T23:39:25.757496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dim = 128\n",
    "num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "data = np.float32(np.random.random((num_elements, dim)))\n",
    "ids = np.arange(num_elements)\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space = 'l2', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "# Initializing index - the maximum number of elements should be known beforehand\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "\n",
    "# Element insertion (can be called several times):\n",
    "p.add_items(data, ids)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "p.set_ef(50) # ef should always be > k\n",
    "\n",
    "# Query dataset, k - number of the closest elements (returns 2 numpy arrays)\n",
    "labels, distances = p.knn_query(data, k = 1)\n",
    "\n",
    "# Index objects support pickling\n",
    "# WARNING: serialization via pickle.dumps(p) or p.__getstate__() is NOT thread-safe with p.add_items method!\n",
    "# Note: ef parameter is included in serialization; random number generator is initialized with random_seed on Index load\n",
    "p_copy = pickle.loads(pickle.dumps(p)) # creates a copy of index p using pickle round-trip\n",
    "\n",
    "### Index parameters are exposed as class properties:\n",
    "print(f\"Parameters passed to constructor:  space={p_copy.space}, dim={p_copy.dim}\")\n",
    "print(f\"Index construction: M={p_copy.M}, ef_construction={p_copy.ef_construction}\")\n",
    "print(f\"Index size is {p_copy.element_count} and index capacity is {p_copy.max_elements}\")\n",
    "print(f\"Search speed/quality trade-off parameter: ef={p_copy.ef}\")"
   ],
   "id": "d8dcdf0ed4a6421f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters passed to constructor:  space=l2, dim=128\n",
      "Index construction: M=16, ef_construction=200\n",
      "Index size is 10000 and index capacity is 10000\n",
      "Search speed/quality trade-off parameter: ef=50\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example with updates after serialization/deserialization:\n",
    "\n"
   ],
   "id": "785d2b125abbee46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:39:47.295610Z",
     "start_time": "2024-05-17T23:39:46.915763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = 16\n",
    "num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "data = np.float32(np.random.random((num_elements, dim)))\n",
    "\n",
    "# We split the data in two batches:\n",
    "data1 = data[:num_elements // 2]\n",
    "data2 = data[num_elements // 2:]\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip\n",
    "\n",
    "# Initializing index\n",
    "# max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "# during insertion of an element.\n",
    "# The capacity can be increased by saving/loading the index, see below.\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "#\n",
    "# M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n",
    "# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "\n",
    "p.init_index(max_elements=num_elements//2, ef_construction=100, M=16)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "p.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "p.set_num_threads(4)\n",
    "\n",
    "print(\"Adding first batch of %d elements\" % (len(data1)))\n",
    "p.add_items(data1)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data1, k=1)\n",
    "print(\"Recall for the first batch:\", np.mean(labels.reshape(-1) == np.arange(len(data1))), \"\\n\")\n",
    "\n",
    "# Serializing and deleting the index:\n",
    "index_path='first_half.bin'\n",
    "print(\"Saving index to '%s'\" % index_path)\n",
    "p.save_index(\"first_half.bin\")\n",
    "del p\n",
    "\n",
    "# Re-initializing, loading the index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.\n",
    "\n",
    "print(\"\\nLoading index from 'first_half.bin'\\n\")\n",
    "\n",
    "# Increase the total capacity (max_elements), so that it will handle the new data\n",
    "p.load_index(\"first_half.bin\", max_elements = num_elements)\n",
    "\n",
    "print(\"Adding the second batch of %d elements\" % (len(data2)))\n",
    "p.add_items(data2)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data, k=1)\n",
    "print(\"Recall for two batches:\", np.mean(labels.reshape(-1) == np.arange(len(data))), \"\\n\")"
   ],
   "id": "193a3b05138fce31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding first batch of 5000 elements\n",
      "Recall for the first batch: 1.0 \n",
      "\n",
      "Saving index to 'first_half.bin'\n",
      "\n",
      "Loading index from 'first_half.bin'\n",
      "\n",
      "Adding the second batch of 5000 elements\n",
      "Recall for two batches: 1.0 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example with a symbolic filter filter_function during the search:\n",
    "\n"
   ],
   "id": "7a051e6f8bc0a043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:40:10.581864Z",
     "start_time": "2024-05-17T23:40:09.736668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = 16\n",
    "num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "data = np.float32(np.random.random((num_elements, dim)))\n",
    "\n",
    "# Declaring index\n",
    "hnsw_index = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip\n",
    "\n",
    "# Initiating index\n",
    "# max_elements - the maximum number of elements, should be known beforehand\n",
    "#     (probably will be made optional in the future)\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "# M - is tightly connected with internal dimensionality of the data\n",
    "#     strongly affects the memory consumption\n",
    "\n",
    "hnsw_index.init_index(max_elements=num_elements, ef_construction=100, M=16)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "hnsw_index.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "hnsw_index.set_num_threads(4)\n",
    "\n",
    "print(\"Adding %d elements\" % (len(data)))\n",
    "# Added elements will have consecutive ids\n",
    "hnsw_index.add_items(data, ids=np.arange(num_elements))\n",
    "\n",
    "print(\"Querying only even elements\")\n",
    "# Define filter function that allows only even ids\n",
    "filter_function = lambda idx: idx%2 == 0\n",
    "# Query the elements for themselves and search only for even elements:\n",
    "# Warning: search with python filter works slow in multithreaded mode, therefore we set num_threads=1\n",
    "labels, distances = hnsw_index.knn_query(data, k=1, num_threads=1, filter=filter_function)\n",
    "# labels contain only elements with even id"
   ],
   "id": "620220484fa0b29e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 10000 elements\n",
      "Querying only even elements\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An example with reusing the memory of the deleted elements when new elements are being added (via `allow_replace_deleted` flag):\n",
    "\n"
   ],
   "id": "941a56c78d3878b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:40:39.340914Z",
     "start_time": "2024-05-17T23:40:38.902106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "dim = 16\n",
    "num_elements = 1_000\n",
    "max_num_elements = 2 * num_elements\n",
    "\n",
    "# Generating sample data\n",
    "labels1 = np.arange(0, num_elements)\n",
    "data1 = np.float32(np.random.random((num_elements, dim)))  # batch 1\n",
    "labels2 = np.arange(num_elements, 2 * num_elements)\n",
    "data2 = np.float32(np.random.random((num_elements, dim)))  # batch 2\n",
    "labels3 = np.arange(2 * num_elements, 3 * num_elements)\n",
    "data3 = np.float32(np.random.random((num_elements, dim)))  # batch 3\n",
    "\n",
    "# Declaring index\n",
    "hnsw_index = hnswlib.Index(space='l2', dim=dim)\n",
    "\n",
    "# Initiating index\n",
    "# max_elements - the maximum number of elements, should be known beforehand\n",
    "#     (probably will be made optional in the future)\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "# M - is tightly connected with internal dimensionality of the data\n",
    "#     strongly affects the memory consumption\n",
    "\n",
    "# Enable replacing of deleted elements\n",
    "hnsw_index.init_index(max_elements=max_num_elements, ef_construction=200, M=16, allow_replace_deleted=True)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "hnsw_index.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "hnsw_index.set_num_threads(4)\n",
    "\n",
    "# Add batch 1 and 2 data\n",
    "hnsw_index.add_items(data1, labels1)\n",
    "hnsw_index.add_items(data2, labels2)  # Note: maximum number of elements is reached\n",
    "\n",
    "# Delete data of batch 2\n",
    "for label in labels2:\n",
    "    hnsw_index.mark_deleted(label)\n",
    "\n",
    "# Replace deleted elements\n",
    "# Maximum number of elements is reached therefore we cannot add new items,\n",
    "# but we can replace the deleted ones by using replace_deleted=True\n",
    "hnsw_index.add_items(data3, labels3, replace_deleted=True)\n",
    "# hnsw_index contains the data of batch 1 and batch 3 only"
   ],
   "id": "537031dc3fb4ae68",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:49:34.924463Z",
     "start_time": "2024-05-17T23:49:34.920075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generating sample data\n",
    "num_elements = 10\n",
    "data = np.float32(np.random.random((num_elements, 3)))\n",
    "\n",
    "# We split the data in two batches:\n",
    "data1 = data[:num_elements // 2]\n",
    "data2 = data[num_elements // 2:]"
   ],
   "id": "97d1dee2121ed0a6",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:49:36.021068Z",
     "start_time": "2024-05-17T23:49:36.013309Z"
    }
   },
   "cell_type": "code",
   "source": "data1",
   "id": "b3b0baf626f11b43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02708375, 0.5794072 , 0.97423446],\n",
       "       [0.9950299 , 0.62489927, 0.01991214],\n",
       "       [0.420828  , 0.94490063, 0.90732855],\n",
       "       [0.35383883, 0.16405845, 0.633536  ],\n",
       "       [0.7151265 , 0.19923542, 0.06441565]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:49:37.092157Z",
     "start_time": "2024-05-17T23:49:37.080610Z"
    }
   },
   "cell_type": "code",
   "source": "data2",
   "id": "af86d33904446e42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80671203, 0.29362255, 0.23393895],\n",
       "       [0.41421872, 0.27104312, 0.6032571 ],\n",
       "       [0.9914797 , 0.88968533, 0.29620737],\n",
       "       [0.36565614, 0.32923284, 0.72991824],\n",
       "       [0.6828612 , 0.15110128, 0.17412773]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
