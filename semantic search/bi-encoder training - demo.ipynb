{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Matching service for e-Commerce\n",
    "\n",
    "- Requirements\n",
    "- Architecture Design\n",
    "- Data \n",
    "- Models\n",
    "- Serving\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/Studeni/amazon-esci-data\n",
    "- ESCI dataset, that includes queries and products with relevance labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "Build a semantic search engine for a e-commerce.\n",
    "\n",
    "# Architecture Design\n",
    "\n",
    "We will use a \"two tower\" architecture and bi-encoder models for this task. \n",
    "\n",
    "**API:**\n",
    "Client interact with the service via REST API. Request body contains a query as free text. Responce contains a list of product ids sorted by relevance to the query.\n",
    "\n",
    "**Inference:**\n",
    "The product data is represented by vector embeddings and stored in the vector in HNSW graph.\n",
    "When user submits the query the query encoder converts the query into a vector. The we call the vector database to retrieve the top N most similar products.\n",
    "The resulting matchset is sorted by the similarity score and returned to the user.\n",
    "\n",
    "**Product Embeddings Generation:**\n",
    "We use HNSW for vector storage. The datasource for product data is a parquet file with the following columns:\n",
    "- product_id\n",
    "- product_title\n",
    "- product_description\n",
    "- product_bullet_point\n",
    "- product_brand\n",
    "- product_color\n",
    "The offline job reads the parquet file and converts the product data into a vector embeddings. The embeddings are stored in the vector database.\n",
    "\n",
    "**Model Training:**\n",
    "We use a bi-encoder model for this task. The model is trained on the training set and evaluated on the validation set.\n",
    "Base model: TK:all-MiniLM-L6-v2\n",
    "Tokenizer: TK:SentencePiece\n",
    "Model validation metrics: ESCI dataset, TK:(MAP@10, MRR@10, NDCG@10, Precision@10, Recall@10, F1@10)    \n",
    "Training algorithm: Contrastive loss\n",
    "\n",
    "**Datasets:**\n",
    "- ESCI dataset, that includes queries and products with relevance labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5653.89s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "shopping_queries_dataset_examples.parquet\n",
      "shopping_queries_dataset_products.parquet\n",
      "shopping_queries_dataset_sources.csv\n",
      "QUERIES:  /Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_examples.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B000MOO21W</td>\n",
       "      <td>us</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07X3Y6B1V</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07WDM7MQQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07RH6Z8KW</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>revent 80 cfm</td>\n",
       "      <td>0</td>\n",
       "      <td>B07QJ7WYFQ</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id           query  query_id  product_id product_locale esci_label  \\\n",
       "0           0   revent 80 cfm         0  B000MOO21W             us          I   \n",
       "1           1   revent 80 cfm         0  B07X3Y6B1V             us          E   \n",
       "2           2   revent 80 cfm         0  B07WDM7MQQ             us          E   \n",
       "3           3   revent 80 cfm         0  B07RH6Z8KW             us          E   \n",
       "4           4   revent 80 cfm         0  B07QJ7WYFQ             us          E   \n",
       "\n",
       "   small_version  large_version  split  \n",
       "0              0              1  train  \n",
       "1              0              1  train  \n",
       "2              0              1  train  \n",
       "3              0              1  train  \n",
       "4              0              1  train  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls ~/datasets/shopping_queries_dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data_path': '/Users/vladimirkroz/datasets/shopping_queries_dataset',\n",
    "}\n",
    "\n",
    "config = {\n",
    "    **config,\n",
    "    'ds_queries': os.path.join(config['data_path'], 'shopping_queries_dataset_examples.parquet'),\n",
    "    'ds_products': os.path.join(config['data_path'], 'shopping_queries_dataset_products.parquet'),\n",
    "}\n",
    "\n",
    "print('QUERIES: ', config['ds_queries'])\n",
    "df = pd.read_parquet(config['ds_queries'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTS:  /Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_products.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "      <th>product_locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B079VKKJN7</td>\n",
       "      <td>11 Degrees de los Hombres Playera con Logo, Ne...</td>\n",
       "      <td>Esta playera con el logo de la marca Carrier d...</td>\n",
       "      <td>11 Degrees Negro Playera con logo\\nA estrenar ...</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Negro</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B079Y9VRKS</td>\n",
       "      <td>Camiseta Eleven Degrees Core TS White (M)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07DP4LM9H</td>\n",
       "      <td>11 Degrees de los Hombres Core Pull Over Hoodi...</td>\n",
       "      <td>La sudadera con capucha Core Pull Over de 11 G...</td>\n",
       "      <td>11 Degrees Azul Core Pull Over Hoodie\\nA estre...</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Azul</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07G37B9HP</td>\n",
       "      <td>11 Degrees Poli Panel Track Pant XL Black</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07LCTGDHY</td>\n",
       "      <td>11 Degrees Gorra Trucker Negro OSFA (Talla úni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11 Degrees</td>\n",
       "      <td>Negro (</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                      product_title  \\\n",
       "0  B079VKKJN7  11 Degrees de los Hombres Playera con Logo, Ne...   \n",
       "1  B079Y9VRKS          Camiseta Eleven Degrees Core TS White (M)   \n",
       "2  B07DP4LM9H  11 Degrees de los Hombres Core Pull Over Hoodi...   \n",
       "3  B07G37B9HP          11 Degrees Poli Panel Track Pant XL Black   \n",
       "4  B07LCTGDHY  11 Degrees Gorra Trucker Negro OSFA (Talla úni...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Esta playera con el logo de la marca Carrier d...   \n",
       "1                                               None   \n",
       "2  La sudadera con capucha Core Pull Over de 11 G...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                product_bullet_point product_brand  \\\n",
       "0  11 Degrees Negro Playera con logo\\nA estrenar ...    11 Degrees   \n",
       "1                                               None    11 Degrees   \n",
       "2  11 Degrees Azul Core Pull Over Hoodie\\nA estre...    11 Degrees   \n",
       "3                                               None    11 Degrees   \n",
       "4                                               None    11 Degrees   \n",
       "\n",
       "  product_color product_locale  \n",
       "0         Negro             es  \n",
       "1        Blanco             es  \n",
       "2          Azul             es  \n",
       "3          None             es  \n",
       "4       Negro (             es  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('PRODUCTS: ', config['ds_products'])\n",
    "df = pd.read_parquet(config['ds_products'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': '/Users/vladimirkroz/datasets/shopping_queries_dataset',\n",
       " 'ds_queries': '/Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_examples.parquet',\n",
       " 'ds_products': '/Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_products.parquet',\n",
       " 'model_name': 'xlm-roberta-base',\n",
       " 'batch_size': 32,\n",
       " 'epochs': 7,\n",
       " 'learning_rate': 2e-05,\n",
       " 'max_length': 128}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "# Merge with existing config dictionary\n",
    "config = {\n",
    "    **config,  # Existing config with data paths\n",
    "    'model_name': 'xlm-roberta-base',\n",
    "    'batch_size': 32, \n",
    "    'epochs': 7,\n",
    "    'learning_rate': 2e-5,\n",
    "    'max_length': 128\n",
    "}\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiEncoder Model\n",
    "\n",
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.product_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.query_encoder.config.hidden_size, 256)  # Project to smaller embedding size\n",
    "        \n",
    "    def encode_query(self, query_inputs):\n",
    "        outputs = self.query_encoder(**query_inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.projection(cls_embedding)\n",
    "        \n",
    "    def encode_product(self, product_inputs):\n",
    "        outputs = self.product_encoder(**product_inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.projection(cls_embedding)\n",
    "        \n",
    "    def forward(self, query_inputs, product_inputs):\n",
    "        query_emb = self.encode_query(query_inputs)\n",
    "        product_emb = self.encode_product(product_inputs)\n",
    "        return query_emb, product_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "class AmazonESCIDataset(Dataset):\n",
    "    def __init__(self, queries_df, products_df, tokenizer, max_length=128):\n",
    "        self.queries = queries_df\n",
    "        self.products = products_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {'E': 1.0, 'S': 0.8, 'C': 0.3, 'I': 0.0}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.queries.iloc[idx]\n",
    "        query = row['query']\n",
    "        product_id = row['product_id']\n",
    "        \n",
    "        # Get product information\n",
    "        product = self.products[self.products['product_id'] == product_id].iloc[0]\n",
    "        \n",
    "        # Handle missing values safely\n",
    "        def safe_get(field):\n",
    "            value = product[field]\n",
    "            return value if isinstance(value, str) else ''\n",
    "            \n",
    "        product_text = (\n",
    "            \"Title: \" + safe_get('product_title') + \"; \" +\n",
    "            \"Description: \" + safe_get('product_description') + \"; \" +\n",
    "            \"Bullet Points: \" + safe_get('product_bullet_point')\n",
    "        )\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        query_inputs = self.tokenizer(\n",
    "            query, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        product_inputs = self.tokenizer(\n",
    "            product_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        label = torch.tensor(self.label_map[row['esci_label']], dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'query_inputs': {k: v.squeeze(0) for k, v in query_inputs.items()},\n",
    "            'product_inputs': {k: v.squeeze(0) for k, v in product_inputs.items()},\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        query_inputs = {k: v.to(device) for k, v in batch['query_inputs'].items()}\n",
    "        product_inputs = {k: v.to(device) for k, v in batch['product_inputs'].items()}\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        query_emb, product_emb = model(query_inputs, product_inputs)\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(query_emb, product_emb)\n",
    "        loss = criterion(cos_sim, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': '/Users/vladimirkroz/datasets/shopping_queries_dataset',\n",
       " 'ds_queries': '/Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_examples.parquet',\n",
       " 'ds_products': '/Users/vladimirkroz/datasets/shopping_queries_dataset/shopping_queries_dataset_products.parquet',\n",
       " 'model_name': 'xlm-roberta-base',\n",
       " 'batch_size': 32,\n",
       " 'epochs': 7,\n",
       " 'learning_rate': 2e-05,\n",
       " 'max_length': 128}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# Load data\n",
    "products = pd.read_parquet(config['ds_products'])\n",
    "queries = pd.read_parquet(config['ds_queries'])\n",
    "\n",
    "# Filter and merge data\n",
    "queries = queries[queries['small_version'] == 1]\n",
    "merged_df = queries.merge(products, on=['product_id', 'product_locale'], how='inner')\n",
    "\n",
    "# Split data\n",
    "train_df = merged_df[merged_df['split'] == 'train']\n",
    "val_df = merged_df[merged_df['split'] == 'test']\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AmazonESCIDataset(train_df, products, tokenizer, config['max_length'])\n",
    "val_dataset = AmazonESCIDataset(val_df, products, tokenizer, config['max_length'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiEncoder(config['model_name']).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m     query_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[63], line 20\u001b[0m, in \u001b[0;36mAmazonESCIDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m product_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get product information\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m product \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducts[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproducts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproduct_id\u001b[49m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Handle missing values safely\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_get\u001b[39m(field):\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/neumans/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch + 1}/{config[\"epochs\"]}, Loss: {train_loss:.4f}')\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'bi_encoder_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neumans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
