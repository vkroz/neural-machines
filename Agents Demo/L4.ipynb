{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Observe Agent Performance\n",
    "\n",
    "In this lesson, you'll add tracing and evaluation to the data agent so you can observe and measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "load_dotenv(override=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>To access <code>requirements.txt</code>, <code>.env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 RAG Triad Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because data agents at their core are performing research and generation tasks, you can apply the RAG Triad to assess the data agent's goal completion.\n",
    "\n",
    "Each step of a data agent's execution seeks to accomplish tasks which you can evaluate via the RAG triad:\n",
    "- research can be evaluated using context relevance\n",
    "- information synthesis can be evaluated with groundedness to assess response accuracy\n",
    "- end-to-end relevance can be evaluated via answer relevance.\n",
    "\n",
    "These metrics are evaluated with an LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "# Use GPT-4o for RAG Triad Evaluations\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Groundedness Feedback Function**\n",
    "\n",
    "The [groundedness feedback function](https://www.trulens.org/reference/trulens/feedback/?h=groundedness_measure_with_cot_reasons#trulens.feedback.LLMProvider.groundedness_measure_with_cot_reasons) expects two inputs:\n",
    "\n",
    "- `source`: list of all contexts retrieved (output of the retrieval step)\n",
    "- `statement`: the statement to check for groundedness, in this case `on_output()` which represents the final answer of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.feedback.selector import Selector\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, \n",
    "        name=\"Groundedness\"\n",
    "    )\n",
    "    .on({\n",
    "            \"source\": Selector(\n",
    "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "                span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS,\n",
    "                collect_list=True\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Relevance Feedback Function**\n",
    "\n",
    "The [relevance feedback function](https://www.trulens.org/reference/trulens/feedback/?h=relevance_with_cot_reasons#trulens.feedback.LLMProvider.relevance) expects two inputs:\n",
    "- `prompt` or user's query (`on_input()`)\n",
    "- `response` or final answer of the agent (`on_output()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context Relevance Feedback Function**\n",
    "\n",
    "The [relevance feedback function](https://www.trulens.org/reference/trulens/feedback/?query=context_relevance_with_cot_reasons#trulens.feedback.LLMProvider.context_relevance) expects two inputs:\n",
    "- `question`: agent's sub-query\n",
    "- `context`: context collected at the retrieval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, \n",
    "             ame=\"Context Relevance\")\n",
    "    .on({\n",
    "            \"question\": Selector(\n",
    "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "                span_attribute=SpanAttributes.RETRIEVAL.QUERY_TEXT,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .on({\n",
    "            \"context\": Selector(\n",
    "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "                span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS,\n",
    "                collect_list=False\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create TruLens session for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database will store OpenTelemetry traces and evaluations (also in opentelemetry format) as events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core.session import TruSession\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "\n",
    "# Initialize connector with SQLite database one folder back\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
    "\n",
    "# Create TruSession with the custom connector\n",
    "session = TruSession(connector=connector)\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Add custom instrumentation to methods that produce intermediate context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import cortex_agent, State\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from langchain.schema import HumanMessage\n",
    "from langgraph.graph import END\n",
    "from langgraph.types import Command\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            ret.update[\"messages\"][-1].content\n",
    "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def cortex_agents_research_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[\"executor\"]]:\n",
    "    query = state.get(\"agent_query\", state.get(\"user_query\", \"\"))\n",
    "    # Call the tool with the string query\n",
    "    agent_response = cortex_agent.invoke({\"messages\":query})\n",
    "    # Compose a message content string with all results new HumanMessage with the result\n",
    "    new_message = HumanMessage(content=agent_response['messages'][-1].content, name=\"cortex_researcher\")\n",
    "    # Append to the message history\n",
    "    goto = \"executor\"\n",
    "    return Command(\n",
    "        update={\"messages\": [new_message]},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import web_search_agent\n",
    "\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            ret.update[\"messages\"][-1].content\n",
    "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def web_research_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[\"executor\"]]:\n",
    "    agent_query = state.get(\"agent_query\")\n",
    "    result = web_search_agent.invoke({\"messages\":agent_query})\n",
    "    goto = \"executor\"\n",
    "    # wrap in a human message, as not all providers allow\n",
    "    # AI message at the last position of the input messages list\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"web_researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Build the graph with added instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from helper import State, planner_node, executor_node, cortex_agents_research_node, web_research_node, chart_node, chart_summary_node, synthesizer_node\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"web_researcher\", web_research_node)\n",
    "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Register the agent with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.apps.langgraph import TruGraph\n",
    "\n",
    "tru_recorder = TruGraph(\n",
    "    graph,\n",
    "    app_name=\"Sales Data Agent\",\n",
    "    app_version=\"L4: Base\",\n",
    "    feedbacks=[\n",
    "        f_answer_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Record agent usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "<p>üö® &nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Your results may differ from those shown in the video.\n",
    "\n",
    "**It's expected for the agent to not answer perfectly**:\n",
    "\n",
    "- If the first query does not chart any results, or plots a chart showing companies A, B and C, or mentions that it doesn't have access to the required data, that's okay. It might be because the Cortex agent decided to choose the Cortex Search tool (access to meeting notes only) instead of the Cortex Analyst tool (access to client deals data). You do not need to re-run the queries.\n",
    "  \n",
    "- The same applies to the second and third queries. If the agent does not return a perfect answer, that's okay. You do not need to re-run it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ </b> The following two queries might take <b>2-5 minutes</b> to output the results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    query = \"What are our top 3 client deals? Chart the deal value for each.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    query = \"Identify our largest client deal, then find important topics in the meeting notes with that company, and find a news article related to the important topics discussed.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Launch the TruLens dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ </b> If the evaluation metrics do not directly show in the dashboard, you can wait a few minutes so that the database is populated with all the values for the evaluation metrics.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "import os\n",
    "str_port = 8001\n",
    "_ = run_dashboard(port=str_port)\n",
    "print(os.environ['DLAI_LOCAL_URL'].format(port=str_port))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
