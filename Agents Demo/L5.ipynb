{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lesson 5: Measure Agent’s GPA\n",
    "\n",
    "Goal-Plan-Act Alignment unlocks useful feedback on how to improve the effectiveness of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "load_dotenv(override=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> 💻 &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Goal-Plan-Act alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents are most effective when acting in alignment with a high-quality plan. For that reason, you can identify common failure modes stemming from misalignment between the Goal, the Plan, and the agent's Actions.\n",
    "\n",
    "Then, through careful criteria and a strong LLM judge, you can develop evaluators to detect these common agent failure modes and assess separable dimensions of agent quality.\n",
    "\n",
    "We will start with some illustrative examples of common failure modes and how we can identify issues in goal-plan-action alignment with an LLM as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpa_eval_provider = OpenAI(model_engine=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 1: Plan Quality\n",
    "\n",
    "The starting point for an agent is its plan. Without a high-quality plan, the agent has little hope of succeeding. You start by assessing the plan to ensure it is well-structured and aligned with the goal.\n",
    "\n",
    "To demonstrate, consider the query: \"Which sales leads should we prioritize this week, and what specific action items should we take for each?\" and the plan below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "goal_and_plan = \"\"\"\n",
    "User Query: Which sales leads should we prioritize this week, \n",
    "and what specific action items should we take for each?\n",
    "\n",
    "Plan:\n",
    "\n",
    "1. Pull all sales leads from the past 12 months from the CRM.\n",
    "\n",
    "2. For the largest 20 leads, compile any notes, call logs, \n",
    "and related tasks from the CRM.\n",
    "\n",
    "3. Summarize each lead’s current stage in the pipeline.\n",
    "\n",
    "4. Present the summary and recommendations in a single table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core.feedback.selector import Selector\n",
    "\n",
    "# Goal-Plan-Act: Plan quality\n",
    "f_plan_quality = Feedback(\n",
    "    gpa_eval_provider.plan_quality_with_cot_reasons,\n",
    "    name=\"Plan Quality\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from helper import display_eval_reason\n",
    "\n",
    "score, reason = f_plan_quality(goal_and_plan)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary:** \n",
    "\n",
    "Why is the first plan low quality?\n",
    "\n",
    "- Vague selection: \"Pull all sales leads from the past 12 months\" lacks urgency constraints tied to the goal.\n",
    "- Weak prioritization: \"largest 20\" ignores lead score, stage urgency, or upcoming deadlines.\n",
    "- Missing actionability: no instructions to create specific next actions or owners.\n",
    "- Output not specific: \"single table\" without required fields tied to the goal.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Specific constraints, measurable outputs, and sequencing tied to goal. The \"better plan\" adds these, raising the score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 523
   },
   "outputs": [],
   "source": [
    "goal_and_better_plan = \"\"\"\n",
    "User Query: Which sales leads should we prioritize this week, \n",
    "and what specific action items should we take for each?\n",
    "\n",
    "Plan:\n",
    "\n",
    "1. Pull all leads with open opportunities from the CRM that have \n",
    "a next action date within the next 14 days or no next action assigned.\n",
    "\n",
    "2. Filter to leads with deal value > $10k or high lead score.\n",
    "\n",
    "3. Sort by deal stage urgency (e.g., close date approaching, \n",
    "at risk of going cold) and potential revenue impact.\n",
    "\n",
    "4. For each prioritized lead:\n",
    "\n",
    "5. Retrieve latest interaction notes, key decision-maker info, \n",
    "and current blockers.\n",
    "\n",
    "6.  Identify overdue or missing action items.\n",
    "\n",
    "7. Propose specific, high-impact next steps (e.g., schedule product demo, \n",
    "send proposal revision, escalate to sales manager).\n",
    "\n",
    "8. Group recommendations into this week’s priority list with owner \n",
    "assignments and deadlines.\n",
    "\n",
    "9. Present results in a table with columns: Lead Name, Value, Stage, \n",
    "Urgency Score, Next Action, Due Date, Owner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "score, reason = f_plan_quality(goal_and_better_plan)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Plan Quality)**\n",
    "\n",
    "- **Bad**: Vague steps, no thresholds, unclear output, missing prioritization.\n",
    "- **Good**: Explicit filters and thresholds, prioritization logic, and defined output schema.\n",
    "\n",
    "\n",
    "In reality, you cannot make direct edits to the plan, nor to the actions of the agent. You can, however, make adjustments to the agent to help guide it towards higher goal-plan-action alignment. In the next lesson, you will get your hands on making such improvements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 2: Plan Adherence\n",
    "\n",
    "Once a high-quality plan is developed, the agent must follow it. Plan adherence checks to make sure the agent's action is aligned with the plan. Consider the following execution trace and the plan you developed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick checklist: Plan Quality**\n",
    "\n",
    "- Tightly aligned to the user goal.\n",
    "- Specific selection criteria and thresholds.\n",
    "- Clear step ordering and ownership when relevant.\n",
    "- Concrete outputs (schema/columns) and success criteria.\n",
    "- Uses the right agents/tools for each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all open opportunities from the CRM without applying a next action date filter.\n",
    "[STEP 2] Applied deal value filter only; skipped the lead score filter.\n",
    "[STEP 3] Sorted leads solely by deal value (descending).\n",
    "[STEP 4] Retrieved latest notes and contact names but skipped blockers.\n",
    "[STEP 5] Listed the CRM’s existing \"next action\" field without review or update.\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, and Next Action.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "plan_and_agent_actions = goal_and_better_plan + agent_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Plan adherence\n",
    "f_plan_adherence = Feedback(\n",
    "    gpa_eval_provider.plan_adherence_with_cot_reasons,\n",
    "    name=\"Plan Adherence\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "score, reason = f_plan_adherence(plan_and_agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why does this trace violate the plan?\n",
    "\n",
    "- Missing the date filter from Step 1.\n",
    "  - Plan: \"next action within 14 days OR no next action\"\n",
    "  - Trace: \"Pulled all open opportunities... without applying a next action date filter.\"\n",
    "- Partial filter in Step 2.\n",
    "  - Plan: value > $10k OR high lead score\n",
    "  - Trace: \"Applied deal value filter only; skipped the lead score filter.\"\n",
    "- Output mismatch.\n",
    "  - Plan: include Urgency Score, Due Date, Owner\n",
    "  - Trace: table has only Lead Name, Value, Stage, Next Action\n",
    "\n",
    "What the evaluator flags:\n",
    "- Each plan requirement should appear in the trace. Omissions above directly lower adherence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "better_agent_actions = \"\"\"[STEP 1] Pulled all leads with open \n",
    "opportunities and either a next action date within 14 days or no next \n",
    "action assigned.\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and identified \n",
    "any blockers.\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context. \n",
    "[STEP 6] Group recommendations into this week’s priority list with owner \n",
    "assignments and deadlines.\n",
    "[STEP 7] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "plan_and_better_agent_actions = goal_and_better_plan + better_agent_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 3: Execution Efficiency\n",
    "\n",
    "Even when acting in logical ways that adhere to a high-quality plan, agents can act in overly defensive ways that reduce efficiency unnecessarily.\n",
    "\n",
    "Evaluating execution efficiency helps to flag redundancies, preventable mistakes, and excessive error handling.\n",
    "\n",
    "Consider a new execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "score, reason = f_plan_adherence(plan_and_better_agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 438
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all leads with open opportunities and either a next \n",
    "action date within 14 days or no next action assigned.\n",
    "    → Retrieved 96 leads.\n",
    "\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "    → Applied filter, yielding 54 leads.\n",
    "\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "    → High-value late-stage leads ranked highest.\n",
    "\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and blockers.\n",
    "    → Retrieved notes from both the CRM API and a cached export for one \n",
    "    lead to “double-check” consistency.\n",
    "\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context.\n",
    "    → Example: Lead A — “Schedule demo and confirm final pricing”; Lead \n",
    "    B — “Follow up on proposal feedback by Thursday.”\n",
    "\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "    → Exported table to both XLSX and CSV formats, though only one \n",
    "    format was requested.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Execution efficiency of trace\n",
    "f_execution_efficiency = Feedback(\n",
    "    gpa_eval_provider.execution_efficiency_with_cot_reasons,\n",
    "    name=\"Execution Efficiency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "score, reason = f_execution_efficiency(agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why is this trace inefficient?\n",
    "\n",
    "- Duplicate work: re-applied the same filter.\n",
    "  - Trace: \"Accidentally re-applied the same filter twice...\"\n",
    "  - Impact: extra compute with no new signal.\n",
    "- Redundant retrieval: double-fetched notes to \"double-check.\"\n",
    "  - Trace: \"Retrieved notes from both the CRM API and a cached export...\"\n",
    "  - Impact: unnecessary network/IO; one source is sufficient.\n",
    "- Unrequested outputs: exported multiple formats.\n",
    "  - Trace: \"Exported table to both XLSX and CSV...\"\n",
    "  - Impact: violates YAGNI; adds time and clutter.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Looks for repeated steps, unnecessary retries, and outputs beyond the plan/request.\n",
    "- All three points above directly lower the efficiency score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Execution Efficiency)**\n",
    "\n",
    "- **Bad**: Re-applies filters multiple times; double-fetches the same notes to \"be safe\"; exports to extra formats not requested; retries on transient warnings without need.\n",
    "- **Good**: Applies each filter exactly once in a single pass; reuses cached results instead of refetching; outputs only the requested format; handles errors proportionally (warn → continue; error → fix once and proceed).\n",
    "\n",
    "Small rewrite example:\n",
    "- Bad: \"Applied deal value filter, then re-applied to confirm.\"\n",
    "- Good: \"Applied combined filters: value > $10k OR high lead score (single pass).\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Mode 4: Logical Inconsistency\n",
    "\n",
    "Agents' actions can suffer from contradictions, ungrounded assumptions, and logical flaws.\n",
    "\n",
    "Let's consider a different execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all leads with open opportunities and either a next \n",
    "action date within 14 days or no next action assigned.\n",
    "    → Retrieved 96 leads, including recent follow-ups and a few older \n",
    "    records from early last year.\n",
    "\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "    → Resulted in 113 leads after applying filters.\n",
    "\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "    → Leads with minimal recent engagement ranked highly due to their \n",
    "    projected close dates in Q3.\n",
    "\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and blockers.\n",
    "    → Several leads show “TBD” for decision-maker but still have active \n",
    "    next steps assigned.\n",
    "\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context.\n",
    "    → Example: Lead A — “Schedule demo and confirm final pricing”; Lead \n",
    "    B — “Wait for proposal feedback before scheduling demo.”\n",
    "\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "    → Due dates range from last week to the end of the current month.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Logical consistency of trace\n",
    "f_logical_consistency = Feedback(\n",
    "    gpa_eval_provider.logical_consistency_with_cot_reasons,\n",
    "    name=\"Logical Consistency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "score, reason = f_logical_consistency(agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why is this trace inconsistent?\n",
    "\n",
    "- Count contradiction after filtering.\n",
    "  - Plan step: filter to leads >$10k OR high lead score.\n",
    "  - Trace: 96 → 113 leads after filter.\n",
    "  - Why it's a failure: filters should not increase the set; this implies an inconsistency.\n",
    "- Action vs state mismatch.\n",
    "  - Trace: decision-maker is \"TBD\" but \"active next steps\" are assigned.\n",
    "  - Why it's a failure: missing prerequisite info for the assigned action.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Numerical sanity across steps; contradictions or impossible transitions.\n",
    "- The two points above directly reduce the consistency score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Logical Consistency)**\n",
    "\n",
    "- **Bad**: Counts grow after applying stricter filters; assigns next steps when decision-maker is \"TBD\"; contradicts earlier statements.\n",
    "- **Good**: Counts decrease or stay the same after filters; next steps match available context; statements remain consistent with prior steps.\n",
    "\n",
    "Small rewrite example:\n",
    "- Bad: \"Resulted in 113 leads after applying filters to 96 leads.\"\n",
    "- Good: \"Filtered 96 → 54 leads based on value > $10k OR high lead score.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Common Failure Modes and Fixes\n",
    "\n",
    "- **Plan Quality**: Ensure the plan is specific, feasible, and tied to the goal. Include ordering and explicit outputs.\n",
    "- **Plan Adherence**: Execute each plan step as written. Apply filters and produce the exact requested outputs; report any deviations.\n",
    "- **Execution Efficiency**: Avoid redundant work and overly defensive retries; do only what is necessary to achieve the goal.\n",
    "- **Logical Consistency**: Keep counts and facts consistent across steps; avoid contradictions and unsupported claims.\n",
    "\n",
    "Use these checklists above to quickly self-audit traces before running full evaluations. \n",
    "\n",
    "**Let's apply these evaluations to the data agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create TruLens session for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from trulens.core.session import TruSession\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "\n",
    "# Initialize connector with SQLite database one folder back\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
    "\n",
    "# Create TruSession with the custom connector\n",
    "session = TruSession(connector=connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from helper import State, planner_node, executor_node, cortex_agents_research_node, web_research_node, chart_node, chart_summary_node, synthesizer_node\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"web_researcher\", web_research_node)\n",
    "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Register the agent with TruLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "    <p>🚨 &nbsp; In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code that registers the agent with TruLens:\n",
    "```python\n",
    "from trulens.apps.langgraph import TruGraph\n",
    "from helper import f_answer_relevance, f_context_relevance, f_groundedness\n",
    "\n",
    "tru_recorder = TruGraph(\n",
    "    graph,\n",
    "    app_name=\"Sales Data Agent\",\n",
    "    app_version=\"L5: Base\",\n",
    "    feedbacks=[\n",
    "        f_answer_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_plan_quality,\n",
    "        f_plan_adherence,\n",
    "        f_execution_efficiency,\n",
    "        f_logical_consistency,\n",
    "    ],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Record agent usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "    <p>🚨 &nbsp; <b>Run Results:</b> In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 1:**\n",
    "``` python\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    query = \"What are our top 3 client deals? Chart the deal value for each.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "records, feedback = session.get_records_and_feedback()\n",
    "print(f\"Query: {records.iloc[0]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[0]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 2:**\n",
    "```python\n",
    "with tru_recorder as recording:\n",
    "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"Query: {records.iloc[1]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[1]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 3**\n",
    "```python\n",
    "with tru_recorder as recording:\n",
    "    query = \"Identify our largest client deal, then find important topics in the meeting notes with that company, and find a news article related to the important topics discussed.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "print(f\"Query: {records.iloc[2]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[2]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Launch the TruLens dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "import os\n",
    "str_port = 8004\n",
    "_ = run_dashboard(port=str_port)\n",
    "print(os.environ['DLAI_LOCAL_URL'].format(port=str_port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
