{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec95313a",
   "metadata": {},
   "source": [
    "## Graded Lab: Reflection in a Research Agent\n",
    "\n",
    "In this graded lab, you‚Äôll implement a simple **agentic workflow** designed to simulate reflective thinking in a writing task. This is one building block of a more complex research agent that will be constructed throughout the course.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Build a three-step workflow where an LLM writes an essay draft, critiques it, and rewrites it. \n",
    "\n",
    "* **Step 1 ‚Äì Drafting:** Call the LLM to generate an initial draft of an essay based on a simple prompt.\n",
    "* **Step 2 ‚Äì Reflection:** Reflect on the draft using a reasoning step. (Optionally, this can be done with a different model.)\n",
    "* **Step 3 ‚Äì Revision:** Apply the feedback from the reflection to generate a revised version of the essay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a473390",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='submission'></a>\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "* All cells are frozen except for the ones where you need to write your solution code or when explicitly mentioned you can interact with it.\n",
    "\n",
    "* In each exercise cell, look for comments `### START CODE HERE ###` and `### END CODE HERE ###`. These show you where to write the solution code. **Do not add or change any code that is outside these comments**.\n",
    "\n",
    "* You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "* Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "* To submit your notebook for grading, first save it by clicking the üíæ icon on the top left of the page and then click on the <span style=\"background-color: red; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19131dc4",
   "metadata": {},
   "source": [
    "Before interacting with the language models, we initialize the `aisuite` client. This setup loads environment variables (e.g., API keys) from a `.env` file to securely authenticate with backend services. The `ai.Client()` instance will be used to make all model calls throughout this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccd6eeb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 161,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import aisuite as ai\n",
    "\n",
    "# Define the client. You can use this variable inside your graded functions!\n",
    "CLIENT = ai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ef4447",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 30
   },
   "outputs": [],
   "source": [
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d6e33",
   "metadata": {},
   "source": [
    "## Exercise 1: `generate_draft` Function\n",
    "\n",
    "**Objective**:\n",
    "Write a function called `generate_draft` that takes in a string topic and uses a language model to generate a complete draft essay.\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "* `topic` (str): The essay topic.\n",
    "* `model` (str, optional): The model identifier to use. Defaults to `\"openai:gpt-4o\"`.\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* A string representing the full draft of the essay.\n",
    "\n",
    "The setup for calling the LLM using the aisuite library is already provided. Focus on crafting the prompt content. You can reference this setup in later exercises to understand how to interact with the library effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbe325d5",
   "metadata": {
    "deletable": false,
    "height": 348,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: generate_draft\n",
    "\n",
    "def generate_draft(topic: str, model: str = \"openai:gpt-4o\") -> str: \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define your prompt here. A multi-line f-string is typically used for this.\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional writer, specializing in writing essays.\n",
    "\n",
    "    You are given a topic and you need to write a draft essay on it.\n",
    "\n",
    "    Topic: {topic}\n",
    "\n",
    "    Write a draft essay on the topic.\n",
    "    \"\"\" \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get a response from the LLM by creating a chat with the client.\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b989100",
   "metadata": {},
   "source": [
    "Run the following cell to check your code is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ee72c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_generate_draft(generate_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a95db",
   "metadata": {},
   "source": [
    "## Exercise 2: `reflect_on_draft` Function\n",
    "\n",
    "**Objective**:\n",
    "Write a function called `reflect_on_draft` that takes a previously generated essay draft and uses a language model to provide constructive feedback.\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "* `draft` (str): The essay text to reflect on.\n",
    "* `model` (str, optional): The model identifier to use. Defaults to `\"openai:o4-mini\"`.\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* A string with feedback in paragraph form.\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "* The feedback should be critical but constructive.\n",
    "* It should address issues such as structure, clarity, strength of argument, and writing style.\n",
    "* The function should send the draft to the model and return its response.\n",
    "\n",
    "You do **not** need to rewrite the essay at this step‚Äîjust analyze and reflect on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1a1cf",
   "metadata": {
    "deletable": false,
    "height": 348,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: reflect_on_draft\n",
    "\n",
    "def reflect_on_draft(draft: str, model: str = \"openai:o4-mini\") -> str:\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define your prompt here. A multi-line f-string is typically used for this.\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional writer, specializing in writing essays.\n",
    "\n",
    "    You are given a draft essay and you need to provide constructive feedback on it.\n",
    "    Follow this guidleine in writing your feedback: \n",
    "    1. The feedback should be critical but constructive.\n",
    "    2. It should address issues such as structure, clarity, strength of argument, and writing style.\n",
    "\n",
    "    Draft: {draft}\n",
    "\n",
    "    Provide constructive feedback on the draft.\n",
    "    \"\"\"\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Get a response from the LLM by creating a chat with the client.\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001ce37a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_reflect_on_draft(reflect_on_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bba56",
   "metadata": {},
   "source": [
    "## Exercise 3: `revise_draft` Function\n",
    "\n",
    "**Objective**:\n",
    "Implement a function called `revise_draft` that improves a given essay draft based on feedback from a reflection step.\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "* `original_draft` (str): The initial version of the essay.\n",
    "* `reflection` (str): Constructive feedback or critique on the draft.\n",
    "* `model` (str, optional): The model identifier to use. Defaults to `\"openai:gpt-4o\"`.\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* A string containing the revised and improved essay.\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "* The revised draft should address the issues mentioned in the feedback.\n",
    "* It should improve clarity, coherence, argument strength, and overall flow.\n",
    "* The function should use the feedback to guide the revision, and return only the final revised essay.\n",
    "\n",
    "In this final exercise, you'll also need to manage the call to the LLM using the CLIENT, as you've practiced in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91595e9",
   "metadata": {
    "deletable": false,
    "height": 280,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: revise_draft\n",
    "\n",
    "def revise_draft(original_draft: str, reflection: str, model: str = \"openai:gpt-4o\") -> str:\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define your prompt here. A multi-line f-string is typically used for this.\n",
    "    prompt = f\"\"\"\n",
    "    You are a professional writer, specializing in writing essays.\n",
    "\n",
    "    You are given a original draft essay and a feedback from reviewer. \n",
    "    You need to write a revised essay based on the feedback.\n",
    "    Follow this guidleine in writing your feedback: \n",
    "    1. The revised draft should address the issues mentioned in the feedback.\n",
    "    2. It should improve clarity, coherence, argument strength, and overall flow.\n",
    "\n",
    "    Original Draft: {original_draft}\n",
    "    Feedback: {reflection}\n",
    "\n",
    "    Provide revised essay.\n",
    "    \"\"\"  \n",
    "\n",
    "    # Get a response from the LLM by creating a chat with the client.\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "318c3d55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_revise_draft(revise_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a10d4c",
   "metadata": {},
   "source": [
    "### üß™ Test the Reflective Writing Workflow\n",
    "\n",
    "Use the functions you implemented to simulate the complete writing workflow:\n",
    "\n",
    "1. **Generate a draft** in response to the essay prompt.\n",
    "2. **Reflect** on the draft to identify improvements.\n",
    "3. **Revise** the draft using the feedback.\n",
    "\n",
    "Observe the outputs of each step. You do **not** need to modify the outputs ‚Äî just verify that the workflow runs as expected and each component returns a valid string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "203c734a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 297
   },
   "outputs": [
    {
     "ename": "LLMError",
     "evalue": "An error occurred: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[0].content', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/aisuite/providers/openai_provider.py:33\u001b[39m, in \u001b[36mOpenaiProvider.chat_completions_create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m transformed_messages = \u001b[38;5;28mself\u001b[39m.transformer.convert_request(messages)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformed_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass any additional arguments to the OpenAI API\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1155\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[0].content', 'code': None}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLLMError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m essay_prompt = \u001b[33m\"\u001b[39m\u001b[33mShould social media platforms be regulated by the government?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Agent 1 ‚Äì Draft\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m draft = \u001b[43mgenerate_draft\u001b[49m\u001b[43m(\u001b[49m\u001b[43messay_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù Draft:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(draft)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgenerate_draft\u001b[39m\u001b[34m(topic, model)\u001b[39m\n\u001b[32m      8\u001b[39m prompt = \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Get a response from the LLM by creating a chat with the client.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mCLIENT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/aisuite/client.py:245\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tool_runner(\n\u001b[32m    236\u001b[39m         provider,\n\u001b[32m    237\u001b[39m         model_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m         max_turns,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Default behavior without tool execution\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Delegate the chat completion to the correct provider's implementation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m response = \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extract_thinking_content(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/vkroz/neural-machines/DL.ai/Agents/venv/lib/python3.12/site-packages/aisuite/providers/openai_provider.py:40\u001b[39m, in \u001b[36mOpenaiProvider.chat_completions_create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLLMError\u001b[39m: An error occurred: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[0].content', 'code': None}}"
     ]
    }
   ],
   "source": [
    "essay_prompt = \"Should social media platforms be regulated by the government?\"\n",
    "\n",
    "# Agent 1 ‚Äì Draft\n",
    "draft = generate_draft(essay_prompt)\n",
    "print(\"üìù Draft:\\n\")\n",
    "print(draft)\n",
    "\n",
    "# Agent 2 ‚Äì Reflection\n",
    "feedback = reflect_on_draft(draft)\n",
    "print(\"\\nüß† Feedback:\\n\")\n",
    "print(feedback)\n",
    "\n",
    "# Agent 3 ‚Äì Revision\n",
    "revised = revise_draft(draft, feedback)\n",
    "print(\"\\n‚úçÔ∏è Revised:\\n\")\n",
    "print(revised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3eb897",
   "metadata": {},
   "source": [
    "To better visualize the output of each step in the reflective writing workflow, we use a utility function called `show_output`. This function displays the results of each stage (drafting, reflection, and revision) in styled boxes with custom background and text colors, making it easier to compare and understand the progression of the essay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50667ba3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 144
   },
   "outputs": [],
   "source": [
    "from utils import show_output\n",
    "\n",
    "essay_prompt = \"Should social media platforms be regulated by the government?\"\n",
    "\n",
    "show_output(\"Step 1 ‚Äì Draft\", draft, background=\"#fff8dc\", text_color=\"#333333\")\n",
    "show_output(\"Step 2 ‚Äì Reflection\", feedback, background=\"#e0f7fa\", text_color=\"#222222\")\n",
    "show_output(\"Step 3 ‚Äì Revision\", revised, background=\"#f3e5f5\", text_color=\"#222222\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367973f7",
   "metadata": {},
   "source": [
    "## Check grading feedback\n",
    "\n",
    "If you have collapsed the right panel to have more screen space for your code, as shown below:\n",
    "\n",
    "<img src=\"./images/collapsed.png\" alt=\"Collapsed Image\" width=\"800\" height=\"400\"/>\n",
    "\n",
    "You can click on the left-facing arrow button (highlighted in red) to view feedback for your submission after submitting it for grading. Once expanded, it should display like this:\n",
    "\n",
    "<img src=\"./images/expanded.png\" alt=\"Expanded Image\" width=\"800\" height=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
