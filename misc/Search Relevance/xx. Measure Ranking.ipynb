{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64ccdd992412d138"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ranking evaluation metrics\n",
    "\n",
    "![Precision and  Recall](img/Precisionrecall.png)\n",
    "\n",
    "Confusion matrix:\n",
    "\n",
    "![Confusion Matrix](img/confusion_matrix_precision_recall_explained.png)\n",
    "\n",
    "## F1 Score\n",
    "The F1 score is a metric that combines precision and recall into a single value, providing a way to measure the balance between them. It is the harmonic mean of precision and recall, giving equal weight to both. The F1 score is particularly useful when you need to compare two or more models that might have different precision-recall trade-offs.\n",
    "\n",
    "$$ \\text{F1} = 2 * \\frac{Precision * Recall}{Precision + Recall} $$\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bb0482ddce8ebe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T19:07:00.332049Z",
     "start_time": "2024-02-23T19:07:00.327693Z"
    }
   },
   "id": "198f9df80b70c411",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Average Precision (MAP)\n",
    "\n",
    "Used for tasks with binary relevance\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f86c3d7c06333"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def score(query, document) -> float:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discounted Cumulative Gain (DCG)\n",
    "\n",
    "Used for tasks with binary relevance\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5233ce864014dc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "60a886b1d36cf887"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine Learning Models for Learning to Rank\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114bae8c4eb546c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "90c5668ccf7cec97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
