{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Ranking algorithms\n",
    "---\n",
    "\n",
    "## \"Hello World\" of search ranking\n",
    "\n",
    "Terms:\n",
    "\n",
    "- Relevance criteria: The basic rules determining how closely an item matches a search query. For example, in a search for 'iPhone,' an item exactly matching 'iPhone' might receive a perfect score (e.g., 1.0), whereas unrelated items, like 'Nike shoe,' would score much lower or even zero. This introduces the concept of relevance in the simplest terms.\n",
    "- Ranking: The process of arranging items by their relevance scores, from the most to the least relevant. This step is crucial for presenting search results in an order that makes sense to the user, starting with the most relevant item. \n",
    "\n",
    "Rankers\n",
    "Ranking Evaluation Metrics\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61bddf17aec1fee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of rankers\n",
    "\n",
    "- Vector Space Models: compute a vector embeddings for each query and document, then compute relevance score as cosine similarity\n",
    "- Learning to Rank (LeToR): the scoring model is ML model that learns to predict a score $s$ given an input $x = (q, d)$ during a training phase where some sort of ranking loss is minimized\n",
    "\n",
    "## Ranking Evaluation Metrics\n",
    "\n",
    "- Mean Average Precision (MAP)\n",
    "- Discounted Cumulative Gain (DCG)\n",
    "- Net Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "$$\n",
    "\\text{MAP} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{1}{m_q} \\sum_{k=1}^{n_q} P(k) \\times rel(k)\n",
    "$$\n",
    "\n",
    "Where $P(k)$ is defined as:\n",
    "\n",
    "$$\n",
    "P(k) = \\frac{\\text{Number of relevant documents among the top } k \\text{ retrieved documents}}{k}\n",
    "$$\n",
    "\n",
    "And the variables are:\n",
    "- $Q$: total number of queries\n",
    "- $m_q$: number of relevant docyments for the $q$-th query\n",
    "- \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "722f7464e376633"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning to rank (LTR)\n",
    "\n",
    "Learning to rank is a class of supervised ML algorithms solving ranking problem. \n",
    "LTR algorithm:\n",
    "- input: (1) query, (2) unsorted list of documents and corresponding feature vectors\n",
    "- output: list of documents, ranked according to ranking criteria \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f397dbfec54c2c5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ranking types\n",
    "\n",
    "- Pointwise ranking\n",
    "- Pairwise ranking\n",
    "- Listwise ranking\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a13352b7886417a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9f6f2b0e702b215"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
