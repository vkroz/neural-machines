{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Search relevance evaluation metrics \n",
    "\n",
    "Consider breaking down into different types of metrics, such as precision, recall, click-through rate (CTR), and mean reciprocal rank (MRR).\n",
    "Discuss the importance of each metric in measuring the relevance and effectiveness of search results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77c4908662af9421"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code snippet will illustrate MAP metrics calculation.\n",
    "We use simplified artificial data, which includes:\n",
    " 1. List of documents simulating search index\n",
    " 2. List of queries\n",
    " 3. List of search results for each query \n",
    "\"\"\"\n",
    "\n",
    "# 1. List of documents simulating search index. Each entry has docId, and title. We have no relevance information at this point, because index exists independsntly from queries.\n",
    "\n",
    "\n",
    "documents = [\n",
    "    {\"doc_id\": 1, \"title\": \"Introduction to Python\"},\n",
    "    {\"doc_id\": 2, \"title\": \"Data Structures in Java\"},\n",
    "    {\"doc_id\": 3, \"title\": \"Web Programming with Python\"},\n",
    "    {\"doc_id\": 4, \"title\": \"ML with TensorFlow\"},\n",
    "    {\"doc_id\": 5, \"title\": \"Deep Learning and ML\"},\n",
    "    {\"doc_id\": 6, \"title\": \"Natural Language Processing\"},\n",
    "    {\"doc_id\": 7, \"title\": \"Software Programming Principles\"},\n",
    "    {\"doc_id\": 8, \"title\": \"Database Design and SQL\"},\n",
    "    {\"doc_id\": 9, \"title\": \"Functional Programming in Java\"},\n",
    "    {\"doc_id\": 10, \"title\": \"Mobile App Development with Java\"}\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    {\"query_id\": 1, \"query\": \"Python programming\"},\n",
    "    {\"query_id\": 2, \"query\": \"Understanding Java data structures\"},\n",
    "    {\"query_id\": 3, \"query\": \"Machine Learning with TensorFlow\"},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T00:22:35.064792Z",
     "start_time": "2024-03-12T00:22:35.058903Z"
    }
   },
   "id": "14174e12e13f3b94",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Average Precision (MAP)\n",
    "\n",
    "The mean average precision (MAP) is a measure of the average relevance of search results. It is calculated by taking the average of the precision values for each query. \n",
    "\n",
    "Let's first define precision. Precision is a measure of the accuracy of the retrieved documents, and it is calculated as the ratio of the number of relevant documents retrieved to the total number of documents retrieved.\n",
    "$$P = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Where:\n",
    "- $P$ is the precision\n",
    "- $TP$ is the number of true positives (relevant documents retrieved)\n",
    "- $FP$ is the number of false positives (irrelevant documents retrieved)\n",
    "\n",
    " Let's have an example which we could compute by hand.\n",
    " For `query_id=1`, the following document are relevant: `doc_id in [1, 3, 7, 9]` because they contain at least one of query keywords: `Python`, or `Programming`.  For the search result containing `doc_id in [1, 2, 3, 4, 5, 6, 7, 8, 9]` precision is: $P=4/9=0.44$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ecb1048d5f3288"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, lets define average precision $AP$). $AP$ is the average of the precision values at each rank, and it is calculated as the sum of the precision values at each relevant rank divided by the total number of relevant documents.\n",
    "\n",
    "$$AP = \\frac{1}{m} \\sum_{j=1}^{m} P(j) \\times rel(j)$$\n",
    "\n",
    "Where:\n",
    "- $AP$ is the average precision\n",
    "- $m$ is the number of relevant documents for a given query\n",
    "- $P(j)$ is the precision at position $j$\n",
    "- $rel(j)$ is an indicator function that is 1 if the document at position $j$ is relevant, and 0 otherwise\n",
    "\n",
    "Let's have an example which we could compute by hand.\n",
    "Given:\n",
    "- query: `query_id=1`\n",
    "- search result: `doc_id in [1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
    "- relevant documents: `doc_id in [1, 3, 7, 9]`\n",
    "- precision at each position (also called 'rank'): \n",
    "\n",
    "| Position | doc_id |  Relevant| P    | AP | \n",
    "|----------|--------|----------|------|----|\n",
    "| 1        | 1      |  1       | 1    | ?  | \n",
    "| 2        | 2      |  0       | 0.5  | ?  | \n",
    "| 3        | 3      |  1       | 0.67 | ?  | \n",
    "| 4        | 4      |  0       | 0.5  | ?  | \n",
    "| 5        | 5      |  0       | 0.4  | ?  | \n",
    "| 6        | 6      |  0       | 0.33 | ?  | \n",
    "| 7        | 7      |  1       | 0.43 | ?  | \n",
    "| 8        | 8      |  0       | 0.38 | ?  | \n",
    "| 9        | 9      |  1       | 0.44 | ?  | \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6f50f3d009a994a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the mean average precision (MAP) is calculated as the average of the average precision values across all queries.\n",
    "\n",
    "$$MAP = \\frac{1}{N} \\sum_{i=1}^{N} AP(i)$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of queries\n",
    "- $m$ is the number of relevant documents for a given query\n",
    "- $P(j)$ is the precision at rank $j$\n",
    "- $rel(j)$ is an indicator function that is 1 if the document at rank $j$ is relevant, and 0 otherwise\n",
    "- The outer sum calculates the average precision for each query, and the final sum calculates the average of these average precisions across all queries.\n",
    "- The MAP value ranges from 0 to 1, with 1 indicating perfect relevance and 0 indicating no relevance.\n",
    "\n",
    "MAP is a useful metric for evaluating the overall performance of a search engine, as it takes into account the relevance of the retrieved documents for each query and provides a single score that summarizes the quality of the search results across all queries.\n",
    "It is important to note that MAP is sensitive to the order of the retrieved documents, as it considers the precision at each rank. This means that the ranking of the search results can have a significant impact on the MAP score, and improving the ranking algorithm can lead to improvements in MAP.\n",
    "\n",
    "\n",
    "Below we illustrate how MAP reflects the relevance. We use the same query \"Python programming\" and we evaluate 3 different search results.\n",
    "Search result is a list of dictionaries, each dictionary has query_id, doc_id, rank, and relevance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "850e7dff6e4dd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAP metric calculation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mean_average_precision(search_results, queries):\n",
    "    # Create a dictionary to store the relevant documents for each query\n",
    "    relevant_docs = defaultdict(list)\n",
    "    for result in search_results:\n",
    "        if result[\"relevance\"] == 1:\n",
    "            relevant_docs[result[\"query_id\"]].append(result[\"doc_id\"])\n",
    "\n",
    "    # Calculate the average precision for each query\n",
    "    average_precision = {}\n",
    "    for query in queries:\n",
    "        query_id = query[\"query_id\"]\n",
    "        relevant = relevant_docs[query_id]\n",
    "        precision = []\n",
    "        for result in search_results:\n",
    "            if result[\"query_id\"] == query_id and result[\"doc_id\"] in relevant:\n",
    "                precision.append(1)  # Document is relevant\n",
    "            else:\n",
    "                precision.append(0)  # Document is not relevant\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T03:42:51.384686Z",
     "start_time": "2024-03-12T03:42:51.360760Z"
    }
   },
   "id": "7918d0958af68c9f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 1:\n",
    "We have 3 different search results for the same query. Each search result has same set of documents, and relevance accurately set to 1 or 0. However ranks in each result are different, and we will demonstrate how MAP reflects the relevance. \n",
    "\"\"\"\n",
    "\n",
    "search_result_1 = [\n",
    "    # Results are set correctly, but ranks are not\n",
    "    {\"query_id\": 1, \"doc_id\": 1, \"rank\": 1, \"relevance\": 1},\n",
    "    {\"query_id\": 1, \"doc_id\": 2, \"rank\": 3, \"relevance\": 0},\n",
    "    {\"query_id\": 1, \"doc_id\": 3, \"rank\": 2, \"relevance\": 1},  # must be ranked as top @1\n",
    "    {\"query_id\": 1, \"doc_id\": 4, \"rank\": 4, \"relevance\": 0},\n",
    "    {\"query_id\": 1, \"doc_id\": 5, \"rank\": 5, \"relevance\": 0},\n",
    "    {\"query_id\": 1, \"doc_id\": 6, \"rank\": 6, \"relevance\": 0},\n",
    "    {\"query_id\": 1, \"doc_id\": 7, \"rank\": 7, \"relevance\": 1},\n",
    "    {\"query_id\": 1, \"doc_id\": 8, \"rank\": 8, \"relevance\": 0},\n",
    "    {\"query_id\": 1, \"doc_id\": 9, \"rank\": 9, \"relevance\": 1},\n",
    "    {\"query_id\": 1, \"doc_id\": 10, \"rank\": 10, \"relevance\": 0}\n",
    "]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T00:18:15.536089Z",
     "start_time": "2024-03-12T00:18:15.530202Z"
    }
   },
   "id": "8a331185741476be",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate the average precision for each query\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary to store the relevant documents for each query\n",
    "relevant_docs = defaultdict(list)\n",
    "for result in search_results:\n",
    "    if result[\"relevance\"] == 1:\n",
    "        relevant_docs[result[\"query_id\"]].append(result[\"doc_id\"])\n",
    "\n",
    "# Calculate the average precision for each query\n",
    "average_precision = {}\n",
    "for query in queries[0:2]:\n",
    "    query_id = query[\"query_id\"]\n",
    "    relevant = relevant_docs[query_id]\n",
    "    precision = []\n",
    "    for result in search_results:\n",
    "        if result[\"query_id\"] == query_id and result[\"doc_id\"] in relevant:\n",
    "            precision.append(1)  # Document is relevant\n",
    "        else:\n",
    "            precision.append(0)  # Document is not relevant\n",
    "    average_precision[query_id] = sum(precision) / len(relevant)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T00:19:04.795915Z",
     "start_time": "2024-03-12T00:19:04.787868Z"
    }
   },
   "id": "d8545ac5a1b39725",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list, {1: [1], 2: [2]})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T00:19:34.710603Z",
     "start_time": "2024-03-12T00:19:34.701623Z"
    }
   },
   "id": "ff1a38e66e3a2589",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
