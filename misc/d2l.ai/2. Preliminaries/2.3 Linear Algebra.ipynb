{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3 Linear Algebra\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.686586Z",
     "start_time": "2023-07-07T18:55:30.739759Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scalars\n",
    "\n",
    "We represent scalar as tensor of dimension=1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(5.),\n tensor(6.),\n tensor(1.5000),\n tensor(9.),\n tensor(False),\n tensor(True))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y, x < y, x > y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.734132Z",
     "start_time": "2023-07-07T18:55:35.690104Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectors\n",
    "\n",
    "Vector is fixed-length array of scalars.\n",
    "We represent vectors as 1-st order tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0, 1, 2]), 3, tensor(0), tensor(2), tensor([0, 1, 2]), tensor(2))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x, len(x), x[0], x[2], x[:], x[-1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.741338Z",
     "start_time": "2023-07-07T18:55:35.733795Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To indicate that a vector contains $n$ elements, we write $\\mathbf{x} \\in \\mathbb{R}^{n}$\n",
    "\n",
    "To get length of the vector we use `len(x)` (above) or `shape` attribute\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = x.shape\n",
    "s\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.749819Z",
     "start_time": "2023-07-07T18:55:35.745301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 2, 2, 2, 3, 2]), True, False, False, False)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.empty(2, 2, 2)\n",
    "t2 = torch.empty(2, 3, 2)\n",
    "\n",
    "s1 = t1.shape\n",
    "s2 = t2.shape\n",
    "\n",
    "s1 + s2, s1 < s2, s1 > s2, s1 == s2, s1 is s2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.819348Z",
     "start_time": "2023-07-07T18:55:35.752268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "s1 = torch.Size([1, 2, 3, 1])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.819828Z",
     "start_time": "2023-07-07T18:55:35.758060Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.3 Matrices\n",
    "\n",
    "Matrix is 2nd order tensor. Denoted by bold capital letters, e.g. $\\mathbf{X}, \\mathbf{Y} etc$\n",
    "\n",
    "The expression $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ indicates that a matrix $\\mathbf{A}$ contains $m \\times n$ real-valued scalars, arranged as rows and columns.\n",
    "\n",
    "Example of matrix:\n",
    "$$\n",
    "\\mathbf{A} = \\[\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & ... & a_{13} \\\\\n",
    "a_{21} & a_{22} & ... & a_{23} \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [2, 3],\n        [4, 5]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.820819Z",
     "start_time": "2023-07-07T18:55:35.764834Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transposing matrix\n",
    "$\\mathbf{B} = \\mathbf{A^{T}}$ , where $\\mathbf{b}_{ij} = \\mathbf{a}_{ji}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 2, 4],\n        [1, 3, 5]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.821315Z",
     "start_time": "2023-07-07T18:55:35.769732Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simmetric matrices: $\\mathbf{A}=\\mathbf{A^{T}}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [2, 0, 4],\n        [3, 4, 5]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.822161Z",
     "start_time": "2023-07-07T18:55:35.781084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[True, True, True],\n        [True, True, True],\n        [True, True, True]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A == A.T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.822785Z",
     "start_time": "2023-07-07T18:55:35.793347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3.4 Tensors\n",
    "\n",
    "Tensor is $n^{th}$ order array.\n",
    "We denote general tensors by capital letters with a special font face (e.g. $\\mathcal{X}, \\mathcal{Y}, \\mathcal{Z}$), and and their indexing mechanism (e.g.$[\\mathcal{X}]_ijk$)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.845291Z",
     "start_time": "2023-07-07T18:55:35.799175Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.5 Basic properties of Tensor arithmetics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor([[ 0.,  2.,  4.],\n         [ 6.,  8., 10.]]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()\n",
    "A, A + B\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.846533Z",
     "start_time": "2023-07-07T18:55:35.805750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  4.],\n        [ 9., 16., 25.]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.847055Z",
     "start_time": "2023-07-07T18:55:35.815055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 2,  3,  4,  5],\n          [ 6,  7,  8,  9],\n          [10, 11, 12, 13]],\n \n         [[14, 15, 16, 17],\n          [18, 19, 20, 21],\n          [22, 23, 24, 25]]]),\n torch.Size([2, 3, 4]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a+X, (a*X).shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.847959Z",
     "start_time": "2023-07-07T18:55:35.823779Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3.6 Reduction\n",
    "\n",
    "Sum of vector elements $\\sum_{i=1}^n x_{i}$\n",
    "Sum of matrix elements $\\sum_{i=1}^m\\sum_{j=1}^n a_{ij}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0., 1., 2.]), tensor(3.))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:35.849008Z",
     "start_time": "2023-07-07T18:55:35.833941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n torch.Size([2, 3]),\n tensor(15.),\n torch.Size([]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, A.sum(), A.sum().shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.042939Z",
     "start_time": "2023-07-07T18:55:35.841275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([3., 5., 7.]), torch.Size([3]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0), A.sum(axis=0).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.049339Z",
     "start_time": "2023-07-07T18:55:35.847485Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 3., 12.]), torch.Size([2]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1), A.sum(axis=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.049942Z",
     "start_time": "2023-07-07T18:55:35.857619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum() # Same as A.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.050595Z",
     "start_time": "2023-07-07T18:55:35.864740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(2.5000), tensor(2.5000))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.051129Z",
     "start_time": "2023-07-07T18:55:35.870017Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.051707Z",
     "start_time": "2023-07-07T18:55:35.877477Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3.7 Non-Reduction Sum\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.052097Z",
     "start_time": "2023-07-07T18:55:35.885587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 3., 12.])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.052524Z",
     "start_time": "2023-07-07T18:55:35.892450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.],\n        [12.]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1, keepdims=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.052998Z",
     "start_time": "2023-07-07T18:55:35.902317Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.3333, 0.6667],\n        [0.2500, 0.3333, 0.4167]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Why keepdim is useful?\n",
    "Because it preserve original tensor structure, and hence we can use it for operations in next steps, e.g. divide original matrix on result\n",
    "\"\"\"\n",
    "A / A.sum(axis=1, keepdims=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.053421Z",
     "start_time": "2023-07-07T18:55:35.907755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 5., 7.]]),\n tensor([[ 0.,  1.,  3.],\n         [ 3.,  7., 12.]]))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cumulative sum\n",
    "\"\"\"\n",
    "A.cumsum(axis=0), A.cumsum(axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.053825Z",
     "start_time": "2023-07-07T18:55:35.918314Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.8 Dot products\n",
    "\n",
    "Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their *dot product* $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$) is a sum over the products of the elements at the same position:\n",
    "$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(32)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example\n",
    "\"\"\"\n",
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([4,5,6])\n",
    "\n",
    "\"\"\"\n",
    "Result will be: 1*4 + 2*5 + 3*6 = 32\n",
    "\"\"\"\n",
    "torch.dot(x,y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.057533Z",
     "start_time": "2023-07-07T18:55:35.923989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(14, 14)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "More examples with numpy\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.dot([1,2,3],[1,2,3]), \\\n",
    "    np.dot([3,2,1],[1,2,7])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.058366Z",
     "start_time": "2023-07-07T18:55:35.942352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 4, 10, 18]), tensor(32))"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "And how to do this hardway, without using built-in dot() function\n",
    "\"\"\"\n",
    "# Step 1 - pairwise multiplication\n",
    "# step 2 - summation\n",
    "s1 = x * y\n",
    "s1, s1.sum()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.059499Z",
     "start_time": "2023-07-07T18:55:35.948369Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.9 Matrix vector products\n",
    "\n",
    "Now that we know how to calculate dot products, we can begin to\n",
    "understand the *product* between an $m \\times n$ matrix\n",
    "$\\mathbf{A}$ and an $n$-dimensional vector\n",
    "$\\mathbf{x}$. To start off, we visualize our matrix in terms of its row vectors\n",
    "\n",
    "$$\n",
    "\\mathbf{A}= \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a}^\\top_{1} \\\\\n",
    "    \\mathbf{a}^\\top_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "$$\n",
    "\n",
    "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n",
    "\n",
    "The matrix-vector product $\\mathbf{A}\\mathbf{x}$ is simply a column vector of length $m$, whose $i^\\mathrm{th}$ element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}= \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a}^\\top_{1} \\\\\n",
    "    \\mathbf{a}^\\top_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\\cdot \\mathbf{x} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    "    \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}\n",
    "\\].\n",
    "$$\n",
    "\n",
    "We can think of multiplication with a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ as a transformation that\n",
    "projects vectors from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$. These transformations are remarkably useful. For example, we can\n",
    "represent rotations as multiplications by certain square matrices. Matrix-vector products also describe the key calculation involved in\n",
    "computing the outputs of each layer in a neural network given the outputs from the previous layer.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor([1., 2., 3.]),\n torch.Size([2, 3]),\n torch.Size([3]))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.to(torch.float32)\n",
    "A, x, A.shape, x.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.059959Z",
     "start_time": "2023-07-07T18:55:35.954093Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 8., 26.]), tensor([ 8., 26.]))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Options to expression for dot operation\n",
    "\"\"\"\n",
    "torch.mv(A, x), A@x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.060357Z",
     "start_time": "2023-07-07T18:55:35.964345Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.10 Matrix-Matrix multiplication\n",
    "\n",
    "Assume we have 2 matrices: $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times n}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a}_{11} \\mathbf{a}_{12} \\hdots  \\mathbf{a}_{1k} \\\\\n",
    "    \\mathbf{a}_{21} \\mathbf{a}_{22} \\hdots  \\mathbf{a}_{2k} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a}_{m1} \\mathbf{a}_{m2} \\hdots  \\mathbf{a}_{mk} \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    ", \\mathbf{B} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{b}_{11} \\mathbf{b}_{12} \\hdots  \\mathbf{b}_{1n} \\\\\n",
    "    \\mathbf{b}_{21} \\mathbf{b}_{22} \\hdots  \\mathbf{b}_{2n} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{b}_{k1} \\mathbf{b}_{k2} \\hdots  \\mathbf{b}_{kn} \\\\\n",
    "\\end{bmatrix}\n",
    "\\].\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $\\mathbf{a_i^\\top} \\in \\mathbb{R^k}$ denote row vector representing the $i^{th}$ row of the matric $\\mathbf{A}$ and let $\\mathbf{b_j} \\in \\mathbb{R^k}$ denote the column vector from the $j^{th}$ column of the matrix $\\mathbf{B}$\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a_1^\\top} \\\\\n",
    "    \\mathbf{a_2^\\top} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a_m^\\top}\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    ",\n",
    "\\mathbf{B} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{b_1^\\top} \\mathbf{b_2^\\top} \\hdots \\mathbf{b_m^\\top}\n",
    "\\end{bmatrix}\n",
    "\\].\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To form the matrix product $\\mathbf{C} \\in \\mathbb{R^{m \\times n}}$, we compute each element $c_{ij}$ as the dot product between the $i^{th}$ row of $\\mathbf{A}$ and the $j^{th}$ column of $\\mathbf{B}$, i.e. $\\mathbf{a_i^\\top}\\mathbf{b_j}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{AB} = \\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a_1^\\top} \\\\\n",
    "    \\mathbf{a_2^\\top} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{a_m^\\top}\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\] =\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_n \\\\\n",
    "    \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_n \\\\\n",
    "    \\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
    "   \\mathbf{a}^\\top_{m} \\mathbf{b}_1 & \\mathbf{a}^\\top_{m}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{m} \\mathbf{b}_n\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "A, B\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.060798Z",
     "start_time": "2023-07-07T18:55:35.970189Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]),\n tensor([[ 3.,  3.,  3.,  3.],\n         [12., 12., 12., 12.]]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, B), A@B\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.061193Z",
     "start_time": "2023-07-07T18:55:35.978817Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3.11 Norms\n",
    "http://d2l.ai/chapter_preliminaries/linear-algebra.html#norms\n",
    "\n",
    "- A norm is a function $\\| \\cdot \\|$\n",
    "- $\\ell_2$ norm measures the (Euclidean) length of a vector.\n",
    "\n",
    "\n",
    "$\\ell_2 = \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} $\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate L2 norm: sqrt(3^2+4^2)=sqrt(9+16)=5\n",
    "\"\"\"\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.061531Z",
     "start_time": "2023-07-07T18:55:35.989373Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manhattan distance. Simple. Less sensitive to outliers.\n",
    "\n",
    "$\\ell_1 = \\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 3., -4.]), tensor(7.))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Manhattan distance\n",
    "\n",
    "L1 = 3+4 = 7\n",
    "\"\"\"\n",
    "u, torch.abs(u).sum()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.061905Z",
     "start_time": "2023-07-07T18:55:35.998196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n 'Torch: 6.0',\n 'Numpy: 6.0')"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Frobenius norm for matices\n",
    "\"\"\"\n",
    "M = torch.ones((4,9))\n",
    "M, f\"Torch: {torch.norm(M)}\", f\"Numpy: {np.linalg.norm(M)}\",\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.062289Z",
     "start_time": "2023-07-07T18:55:36.018220Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([2,3,4])\n",
    "len(t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.062725Z",
     "start_time": "2023-07-07T18:55:36.020573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nIt always corresponds to the length oz first (i.e. zero) axis. For matrix it will be number of rows\\n'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain axis of X? What is that axis?\n",
    "\"\"\"\n",
    "len(torch.ones(2,3,4)), len(torch.ones(1)), len(torch.ones(4,4,4)),len(torch.ones(4,2,3)),\n",
    "\n",
    "\"\"\"\n",
    "It always corresponds to the length oz first (i.e. zero) axis. For matrix it will be number of rows\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.063200Z",
     "start_time": "2023-07-07T18:55:36.024312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 1., 2.],\n         [3., 4., 5.]]),\n tensor(15.),\n tensor([ 3., 12.]))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?\n",
    "\"\"\"\n",
    "A, A.sum(), A.sum(axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T18:55:36.334858Z",
     "start_time": "2023-07-07T18:55:36.035442Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
